## Directory Structure ##
./
    .env
    .env copy
    .gitignore
    analysis_interrogate.py
    analysis_LLM.py
    analysis_main.py
    analyzer.py
    api_utils.py
    clip_analyzer.py
    CodeSheetConfig.ini
    Code_From_CWD.txt
    config.py
    Dir_Structure.txt
    image_utils.py
    Info.txt
    json_utils.py
    LLM_CodeSheetV2.exe
    LLM_Prompts.json
    logging_setup.py
    output.json
    README.md
    results.json
    test.png
    TEST.py
    utils.py
    .git/
        COMMIT_EDITMSG
        config
        description
        FETCH_HEAD
        HEAD
        index
        ORIG_HEAD
        hooks/
            applypatch-msg.sample
            commit-msg.sample
            fsmonitor-watchman.sample
            post-update.sample
            pre-applypatch.sample
            pre-commit.sample
            pre-merge-commit.sample
            pre-push.sample
            pre-rebase.sample
            pre-receive.sample
            prepare-commit-msg.sample
            push-to-checkout.sample
            update.sample
        info/
            exclude
        logs/
            HEAD
            refs/
                heads/
                    main
                remotes/
                    origin/
                        main
        objects/
            00/
                5a626720704718cb7698d4871ed3c750a06e91
                d1f703ed65a512edaa66502a788801006f11d4
            01/
                363af75eef02ef1dfe65d39d7dc2927ec113b8
            04/
                c9e599e36b7f2ced28a31af4a36b88b9b1bff3
            05/
                858c7d0808f3d64570a77f8aca34823fe02421
            06/
                f0cfda7d8b1490d0602d0a251ba23ddd083e1a
                f62320db2a1952d218ef234b563eb22522f09d
            07/
                8e73966417cd5b7f5e335aacf4d325ad502a51
            09/
                b55fca364bc89b6bda729320fe45e2e4a61f5e
            0a/
                7ac1a4e73c95918a4f207740529fad15bd6f79
                8dd6a5ab35a1f1efa50b92cd670fe81767c901
            10/
                5dbceb4f9ea2e2cc11e2d0bd06cea2954b4f0e
            11/
                0ca06e42e4d77e52122b762f20c3b5c74847c8
            12/
                2d2a82ce9f57349263c2f1d912401b34afbb6f
            14/
                143012ca8153a65b874692a7593376d731ea99
                a0be5e43b999d23e0afc2b63a48509e5c03698
            15/
                38383cb4d5ab9b549742274fd7b5842f8d96be
            17/
                1c35e99f5fabe2d12bf39568f56ef5e4bda197
            19/
                7f42d48dbe921a275f108f85233d16d5c66462
            1a/
                62411753d4eb1b19712bffd299a390c08121e4
            1b/
                46ca506b3812182d9e11e97555226ee8c3d608
                57956dbad23f674846e8b4fb07e6c65dafe321
                78c71243e361a5122527e13e7731f566781ce5
            20/
                8fdd5f53a04175ce4eeca994ff0a0373654475
            21/
                8087db7dc42d4b59095e693f098aa95afca11e
            23/
                12dfd9e73360e76dafa950adf8b411338fe109
            25/
                944b0685067ed155f0c7e5088310ae3f214f1b
            28/
                1d703ca8a5e6418ab67f938ae8057f971bf80d
            29/
                2f3193601d86bbf98bed2a01c651b01012eef3
                b3a73395707067a3a2483a9d68efe88eeec872
            2a/
                cc6e4ed4d6f4d7f288927a6ccdd401279e7772
            2c/
                edc9273b09e8e813e7d478cfcbd1a8ac965a9c
                fd812683bb4fdf6001036cd2e00a094d8ecbce
            2e/
                a313fbd02d944d30b438b31f80bc4e1dc2db80
                b037b49e9a5830d449588525ba62f54a8d4372
                fe36e980b21d7aa98fb5a27d9d53893a7da829
            2f/
                0f77ac982253b588dc76aafc91f519fdb729d3
                85059977ee9d5233b15cb94ea0c4a29816e365
            32/
                62d8691b43dbf277aed243f0cd778340dc29ca
            33/
                29cc6a54226af9ec4b353625d22853bcd36ba2
                5a859abbddcf85ddcc458b7f3b7db482838cf4
            34/
                6d5544a94821b6db80a232d0ae73287ce828b7
            36/
                2f155a894743bb00775ee5a5f9269accdf67cf
            38/
                985789b4be37afb3546c4729a20a6ac9de3a1b
            39/
                e0e3c2264a0184bd2f34636f2d4477baf57ea0
            3a/
                0f13e0500d93ae06abecdf94050e2d6af326b4
            3b/
                104d6b2a1f8eb2df76cae16e0e0d2091e63a12
            3c/
                da5ea0c5fc4b06b99414f0faeb6df98c530c96
            3f/
                2c76c4b88c79d6df4de57c222b671bc4acdd34
            41/
                384258cd3593f174e6af4c3323b3b0b6b501b7
                f1921a29240ed1564acec36df3cf39716e2476
            42/
                ddc18de1559f43c55209177e90e32348e9eac4
            43/
                1a30a922f1ef2c666f42e8fe7b9ca3504cb1f3
                b81a09f33338aee0ea83abe9d90c3e700d116f
            44/
                27e5315e9b6138c2aabe66b2b7ee161b977c8b
                2c4d96779eb6d34c940211adcc09df34bb7316
                ae09b40013728d0071e07ed89b660dd14cbd0c
            46/
                accc6aed2be2b2c358fbfd3d9846a45e42e15b
            48/
                5442c29d4a13e963f0e1e2e9482bb5af519d3e
                bb04c88b1a5c05dd2695db479040d86fc830b6
            4a/
                9a2d86738ed2a39105a3788850ef385aa640a7
                ed745baa7e38a63bb7302b5f640855cde8e679
            4b/
                14f6ceaf1225250abff5fdb7aea8d89e2ecaf9
            4c/
                05e6aa8483cd23c67c2397bf8a59eb4809ec95
                cfe63fe69ceca170be867ad14d31f736231797
            4e/
                74bcde14be10fed65ed87d21cc920519e19edc
                86f0211bbc4e2e5e6a9f6afb2c49fef903d894
                9effc7b30be4839161c70452622a11e443e96c
            51/
                0adb0c4449adacdc6c45773510aa1ff9b3589b
                d1d770fff8a73b002e5e3e215755aaf1bdbe1a
            52/
                694e94af0c0105262746eeb9181a660eeed06c
            53/
                77a3dd45c706f304d0d2499685ceae792c492d
            54/
                a83c5cc6112abb8e0ee6a85dc3f4d5c4228415
            55/
                502ad96551fea27292a1205fb34b97760a0c0d
            56/
                5030af84d2c8ab2949905806a6c99dd0776446
                f880e2a49b1647a160ee5e8819d914567abf60
            57/
                904a1fc9edeb43977fd1bb07bdd607f3441c93
            58/
                5cf19e2520de110356c9a3745633c664e74045
            5a/
                78e63225f64f6875a303048540a3ef790e2d34
                ac8448b1fa20361f792198006ca2a0212a2f25
                d1957cc8fcb0152d493eb321bad0d75c00e585
            5b/
                ab203d7fd24bb58540da9e8c610f7f54003200
            5f/
                adfa6d90e9bc9741cdd40755773d957a245095
            61/
                0fe02b5f1700c761a78c54db55545c89642ded
                5bb3be1143988082690293e03c5b36e6747a58
            62/
                00b8ca65bb131e27dd3a0c492cf80e2855c6b5
            63/
                d63d0fc603bb461288c724a28f00b70c49b4ec
            65/
                716bf88866d42cb1f5cef4c514887d19445afb
            66/
                eed65d9f2ed8ed53f49e626461493a48f6313c
            67/
                b5577a351307a2e9b975c5734b135437e9db54
            68/
                725b81f7ad363fc274f4b002eba39ac5953901
            69/
                c8d00bcf98864c3c91e4a3296ec0d6b386b0a9
                dc7b841248c3c73ca0fc890d9de37e1a44cdaa
            6b/
                769b87c40351060454e1b84a9daa5cdb524aad
                c36623b07e74449ac34659003c661d6a4dfb1a
            6d/
                18c9c2c08f8cbd79e8c352d0e2973a22427679
                45e7981c7d25399e5a276ae2a3d72c369ff9a7
            6e/
                01edc16812a6ea645c80ead68687281e83f77d
            6f/
                85429316bcfb71d295e81f75abffdfbd97963d
            70/
                3d65a3bb61c4f017136bd9a9bac25bba05dba9
            71/
                62c335a318aed858d7779f04a01790f964921b
            72/
                70cc991c88a7347b64e3ec42a84a9734ad6259
                e4833b738a5572dbedad149277f07720e5a50e
            73/
                15aab94c64699b12a627f26dacb02f8c9c0d78
            75/
                51de04ed3eba2c8544e06b4b0d38fce0b9674e
            78/
                a222d0efa3298d595a599855100f15a2d6db43
            7b/
                50f2ae5e7f7211dc1f6949e6312f11512989aa
            7d/
                1c6434bb60de5226147d616ef01015e76000f0
            7e/
                55972ad34b1dd3bdbc8cf86d0843a477693bb4
                8c35f7895bbf25ce2ee4261ec452edbb208f03
            7f/
                762bf43d516a640ff487e2ac251b67537ab61b
            80/
                ce59828d7a25f49d11904781121bd47517b323
            81/
                34429f5efbc0ed84cbf64affa2cf35a34bdd25
            83/
                3543101dbaae482f79d04f022be1faeca1a3a3
            84/
                078c4469f2b1670f2e6a6a6ad2fd292484a161
            85/
                33268ec2390c8eeae927d4dabc425f81aa35c6
                f7b1098fbd59ed03a3881be001556d46fc845e
            87/
                60912767c664e8f900950f3c2d331a6476adbb
            8a/
                428ed09dbdd3da9e3e27f1f1cd091d31c405cd
            8b/
                a4bd4f7da2fbf6cb2a4e0d0f41f9125888e15a
            8e/
                51200d0afc7bb0a5ef144342941835a007ea23
            8f/
                af0c445edd99ea8ee033321768f772f2694471
            90/
                c5250571b418089881d0b370cb55a907b017f4
                e84a4dffe271af2d9148f99d0418dedc9af356
            91/
                935678916e0f61138543914e188a355a904266
            92/
                cf921f497e678e14658c4c7de4a81e28df60e6
            96/
                3119b18d3e674dc39dd50cd947766f8804175a
                95b74168640e69e1d274691349cd936f627984
                9cfc2a0dd47b2dfd2b1841a6415fc4c0b08cfd
            97/
                149e0b7f0469fbc9779b9cce95d01cf7a5936d
                40c423c2b951eef1018023699fad291c680400
                9688056d0242ac45154065bdab70483d1d7099
            98/
                1b97272287ef06f7cbd808f559a9b39add68ad
                56dc3c3d24c7ad2c96c0189413f03b2d699769
            9a/
                c7c4daf231acb6017e5665dc60f32218cd2851
            9b/
                6cec43bcc4c94c51d1e955baf6a3dd44378bd0
                866dd1736955f7a235f5dbc15a6fddf563e493
            9c/
                db5cf19b5cf482f2d42c36b834006ef1c2958d
            9d/
                746a318e38dac971deb5009d8512325d1da827
            9e/
                33a1c38731b71795a59697d9842760f9fdd026
                7a6c12e8123ad48be17ee0a3b92210720e73ef
            a0/
                3fa8673995438b9224ef5b6304da6d775a8adf
            a1/
                8178521adb7ecce93bbb082ea4f395d65b3a9d
            a3/
                027d680fb49487f69498a9c66d1bb9efdb0237
            a4/
                5b147aff201a09ee8e9d2d13b10bfcb8b2e8ff
                9e8c9c8c016779b4575d426c8fd16eb865c8ab
            a7/
                1511e577aba3df3db583474232e2c5889bc739
                3d021210ceac20c1374600e329e48f6a2f4ec1
                6ee0ba0ba647f14d7dc90f837976707974d2ee
                ace3e8665fe8f03447cb7cf5d724e9f86e374a
                db5c341cae337b13d55a0c0bb12d0e8efb1d46
            a8/
                2338dbb45d6d525a6a011363143c0830b93f38
            aa/
                658af92755e48a2fbdb812f4190715a2a77cf0
            ac/
                ec98edd4b0631ae63d7edb6f8734aaa39f563e
            ad/
                30723467ad488ee00ac303a7ec9320c5fe1efd
                67d44dc5247004866e1d24e8f9b08e4416dbbc
                b7e5d4e5c042d57d92efc7af0bb20a4fc96400
                f90431bdf5e7d1e30777331c7cc97886889172
            ae/
                ab11aae483224397c149b7475b70d138065749
                ba0cf0a3eec7eea9ab7772ffe930d2ca397510
            af/
                34c0b5a9696a6c974eb00da64c9180a776be28
            b0/
                370f431754ff6a2edc5740791cd1e57fce1f96
            b1/
                106780ffbbf9555e07f3c2897fd5cf666b5184
                23e9eafb63e1e877662a6901e188748f1b4741
            b2/
                2d20743c0b25f7566f05852a7e9d637f645f7f
                5719bebf12cbe95293eb2eb6bca6f931a2fdec
                f60574e41d7d2978e9cdd927c23cc1ec8e5ba7
            b3/
                84f7fb77024b52f25c6ad199cf21c13c88e8a5
            b4/
                fc0b239360654a3a8648c9ccde04146861d5e0
            b5/
                8088ca3135ea3e2f7d4b02ee2265545c46b989
            b8/
                4c0c4337f120d22493446a26fd784f6b850d7d
                a9fc9f9db35bef338f678d6ff9e45ce68e111d
                ad5967c8bdec65edc17f0b58ec8ac27f3262d3
            ba/
                841f3b2a779a727640c91d6afafc70eff13412
            bb/
                542e24557ff8f99a31532d7d3e8489886093d0
            bc/
                fa9923e522703fea3e83c4483b1f345ae6dfab
            bd/
                f97c1eae29597d19e85db9e9e8a77a02fc45fe
            be/
                683c86f5317a1ee8214a8e95e23f852a242cf6
                f32062e09c49f873a63891a685e302572e475d
            bf/
                72aca796a9db2c5e0985c287d273deb9268c8f
            c1/
                3ed53a153e9d2023db56af9fe8d12b9a717d89
                e0435a7842299b1234dfed5764ab273502b3b9
            c2/
                4eaeaff48e0ebdc13ffbca84dd7e132fedc7d5
                f9d504ac572b3d45b96b031e585bf14991da22
            c5/
                aab2196d73c5947a78a3747f760c55c4b818cb
            c6/
                a4695b849273244563318679a53bf779a2d02b
                b5f3dceaec9e009841aedca32685ee0b8e5f85
            c8/
                7cd4553b546b5196a662fcc0f3bf80d8cf907f
                aa4adfc63f6bc81f26bde89d9d7b6e21eec76b
            c9/
                48b53fa029eb204d6f2970e3bddadf680ae8f8
            ca/
                26d08467178daef794f2ba1f2b5bad6e003db0
                4bbc13a7698526ed65bac1e5f89a00d5c0ed27
            cb/
                42f58cff2c6baec69c9321b5c07a0642158a2a
            cc/
                5a3e94aab3fe6c863db25169cdce6911ea166e
            cf/
                9f31dfd6de924a0bfe4616b9eeba53b8abd9ae
            d1/
                4e866b0c7fa1dac7fe3bd32198967a9dfa7821
            d3/
                fba730d874eff953bdc1e742c2d8fc1b46a3e1
            d4/
                8ecf06614c250592352cf7a48f84ae2aa11b91
            d6/
                ef30e2314f2251d3c873ae82703ec1823e4e78
                f9ac1143e5a83fb9098f1c9d9962b4a4ff58a3
            dc/
                cdfff6a508dced40da8f154dc86d45b09d1ba9
                e8c2758e73889730abc761bb36ce1e349ebcb6
            de/
                06f27cf0925950ed2c0d31e3f75d17aaf29a6e
            df/
                ff068b1634dd3ab77befb0ab3bc80f1ac44386
            e1/
                caaded6e9e4d6b3212960f062ae5e32f61367a
            e4/
                a07393b7876f6bfd0a4566551f1be18f61f5cd
            e5/
                00ac1a8b82e6ead49fafb75e71539ae3b68d90
                e7a0c50af4d36714fe39ef03cbb2a0c6d5ebad
            e6/
                ac88c99108d0a3c782c62f4ca1782ccdf90e44
                f53c5789c536767397f11de18cb4135fe9712c
            e7/
                149f6a5e0642d4a0feb1d2b19e240e898bac91
            e9/
                c598f088daf091434a18a9d40b8d850a17a979
            ea/
                26810b53e5cc2f06b7ba0ed4e00090f669748a
            eb/
                2519db0f8902504097140f8158ce293c2e9110
            ec/
                cf94bbb9e32d164fda7f37cf066bc5c7c68dc3
                e5edab650abc7f8644c76cb807251ba0ec8512
            ed/
                622d6f6c5b5c3cbd39a7fbbf9c92d0e9eaad27
                7a416949d6e9e2210f9b6b0ce80567b88ebdf1
            ee/
                87a46978f18fdb25851e35f51c6b0a81e82827
            ef/
                6bce238e104eb332cf32412035c0d075aba6a0
                bc887cb2beb33f22b5c98d944d76d8e3e71d8e
            f0/
                806596afc054a21f4d41303b3be69969c54dbf
            f1/
                8c4a896be7830850647f6f78e416193fcaed37
            f2/
                654353c1fc0e2f13dbf7511e38c2cbe425a2cc
            f3/
                03345aab6ab56c019888d5fe3589440af8bbaf
            f4/
                36b0b3a7402b6495e713ceba9d54df02d71262
            f7/
                0003326dacac408227f6f763c2464a122d61c3
                b494fbdf6fdfd3a858c31774d5a950ac88128a
                c9ff214039b361b36b444292bd03950750fcb9
                f58c77405e25b3061c8ed0bcccff1c96eb4847
            f8/
                eaa8a3848d53bf6b2337c92aafc684912cb49d
            fb/
                337f1aa3ee525111ef19e66f9e098da8b21c27
                60d4b3b1ffe94e3056b9ae7d827709aeb7bd5d
            ff/
                2668437496ced2701b56bf8cd2992c76575fef
                362cbcbd5616d12a6f2dcd4349713b66590e0a
                381e979b6d2a7ec5d3540f40a199c1205f117d
            info/
            pack/
        refs/
            heads/
                main
            remotes/
                origin/
                    main
            tags/
    Images/
        Group/
            Group (1).png
            Group (2).png
            Group (3).png
            Group (4).png
            Group (5).png
            Group (6).png
            Group (7).png
    Output/
        analysis.log
        api_clip.log
        api_communication.log
        api_llm.log

## Main Directory ##

###### FILENAME: analysis_interrogate.py ######

"""
Parameters:
    image_path (str): (Required) Path to the image file.
    --api_base_url (str): Base URL of the CLIP API. Default is http://localhost:7860.
    --model (str): Model name for analysis and prompt generation. Default is ViT-L-14.
    --modes (List[str]): Prompt modes to generate. Choose from 'best', 'fast', 'classic', 'negative', 'caption'. Default is all.
    --output (str): Filename for the JSON output. Default is analysis_results.json.

Example:
    python analysis_interrogate.py test.png --modes best fast --output output.json
"""

import requests
import base64
import os
import json
import argparse
from typing import List, Dict, Any
from dotenv import load_dotenv

# Load environment variables from .env file
load_dotenv()

# Load status messages from .env or set default words
EMOJI_SUCCESS = os.getenv("EMOJI_SUCCESS", "SUCCESS")
EMOJI_WARNING = os.getenv("EMOJI_WARNING", "WARNING")
EMOJI_ERROR = os.getenv("EMOJI_ERROR", "ERROR")
EMOJI_INFO = os.getenv("EMOJI_INFO", "INFO")
EMOJI_PROCESSING = os.getenv("EMOJI_PROCESSING", "PROCESSING")
EMOJI_START = os.getenv("EMOJI_START", "START")
EMOJI_COMPLETE = os.getenv("EMOJI_COMPLETE", "COMPLETE")

def encode_image_to_base64(image_path: str) -> str:
    """
    Encodes an image file to a base64 string.

    Args:
        image_path (str): The path to the image file.

    Returns:
        str: The base64 encoded string of the image.
    """
    try:
        with open(image_path, "rb") as image_file:
            return base64.b64encode(image_file.read()).decode('utf-8')
    except Exception as e:
        raise FileNotFoundError(f"Unable to encode image. Error: {e}")

def save_json(data: Dict[str, Any], filename: str):
    """
    Saves data to a JSON file.

    Args:
        data (dict): The data to save.
        filename (str): The name of the file to save the data to.
    """
    try:
        with open(filename, 'w') as f:
            json.dump(data, f, indent=4)
        print(f"{EMOJI_SUCCESS} Saved output to {filename}")
    except Exception as e:
        raise IOError(f"Failed to save JSON to {filename}. Error: {e}")

def analyze_image(image_path: str, api_base_url: str, model: str, timeout: int = 60) -> Dict[str, Any]:
    """
    Sends an image to the CLIP API for analysis.

    Args:
        image_path (str): The path to the image file.
        api_base_url (str): The base URL of the API.
        model (str): The model name to use for analysis.
        timeout (int): The timeout for the API request (default is 60 seconds).

    Returns:
        dict: The JSON response from the API containing analysis results.
    """
    image_base64 = encode_image_to_base64(image_path)
    
    payload = {
        "image": image_base64,
        "model": model
    }
    
    headers = {"Content-Type": "application/json"}
    
    try:
        response = requests.post(
            f"{api_base_url}/interrogator/analyze",
            json=payload,
            headers=headers,
            timeout=timeout
        )
        response.raise_for_status()
        return response.json()
    except requests.RequestException as e:
        raise ConnectionError(f"API request failed during analysis. Error: {e}")

def prompt_image(image_path: str, api_base_url: str, model: str, modes: List[str], timeout: int = 60) -> Dict[str, Any]:
    """
    Sends an image to the CLIP API to generate prompts.

    Args:
        image_path (str): The path to the image file.
        api_base_url (str): The base URL of the API.
        model (str): The model name to use for generating prompts.
        modes (List[str]): List of modes for prompt generation (e.g., 'fast', 'best').
        timeout (int): The timeout for the API request (default is 60 seconds).

    Returns:
        dict: The JSON response from the API containing prompt results.
    """
    image_base64 = encode_image_to_base64(image_path)
    prompts = {}

    headers = {"Content-Type": "application/json"}

    for mode in modes:
        payload = {
            "image": image_base64,
            "model": model,
            "mode": mode
        }

        try:
            response = requests.post(
                f"{api_base_url}/interrogator/prompt",
                json=payload,
                headers=headers,
                timeout=timeout
            )
            response.raise_for_status()
            prompts[mode] = response.json()
        except requests.RequestException as e:
            print(f"{EMOJI_ERROR} Failed to get prompt for mode '{mode}'. Error: {e}")
            prompts[mode] = {"error": str(e)}
    
    return prompts

def parse_arguments() -> argparse.Namespace:
    """
    Parses command-line arguments.

    Returns:
        argparse.Namespace: Parsed arguments.
    """
    parser = argparse.ArgumentParser(
        description="Process images to generate prompts and perform analysis using the CLIP API."
    )
    
    parser.add_argument(
        "image_path",
        type=str,
        help="Path to the image file to be processed."
    )
    
    parser.add_argument(
        "--api_base_url",
        type=str,
        default="http://localhost:7860",
        help="Base URL of the CLIP API (default: http://localhost:7860)."
    )
    
    parser.add_argument(
        "--model",
        type=str,
        default="ViT-L-14",
        help="Model name to use for analysis and prompt generation (default: ViT-L-14)."
    )
    
    parser.add_argument(
        "--modes",
        type=str,
        nargs='+',
        choices=['best', 'fast', 'classic', 'negative', 'caption'],
        default=['best', 'fast', 'classic', 'negative', 'caption'],
        help="Prompt modes to generate. Choose from 'best', 'fast', 'classic', 'negative', 'caption'. Default is all."
    )
    
    parser.add_argument(
        "--output",
        type=str,
        default="analysis_results.json",
        help="Filename for the JSON output (default: analysis_results.json)."
    )
    
    return parser.parse_args()

def main():
    args = parse_arguments()
    
    image_path = args.image_path
    api_base_url = args.api_base_url
    model = args.model
    modes = args.modes
    output_filename = args.output
    
    # Verify that the image file exists
    if not os.path.isfile(image_path):
        print(f"{EMOJI_ERROR} Image file '{image_path}' not found.")
        return
    
    results = {
        "image": os.path.abspath(image_path),
        "model": model,
        "prompts": {},
        "analysis": {}
    }
    
    # Generate prompts
    if modes:
        print(f"{EMOJI_PROCESSING} Generating prompts for modes: {', '.join(modes)}")
        try:
            prompts = prompt_image(image_path, api_base_url, model, modes)
            results["prompts"] = prompts
        except Exception as e:
            print(f"{EMOJI_ERROR} An error occurred during prompt generation: {e}")
    
    # Perform analysis
    print(f"{EMOJI_PROCESSING} Performing image analysis.")
    try:
        analysis = analyze_image(image_path, api_base_url, model)
        results["analysis"] = analysis
    except Exception as e:
        print(f"{EMOJI_ERROR} An error occurred during analysis: {e}")
    
    # Save results to JSON
    try:
        save_json(results, output_filename)
    except Exception as e:
        print(f"{EMOJI_ERROR} Failed to save results: {e}")

if __name__ == "__main__":
    main()



###### FILENAME: analysis_LLM.py ######

#!/usr/bin/env python3
"""
LLMAnalyzer Standalone Script

This script analyzes an image using a selected LLM (Language Model) API.
It accepts prompts directly or via prompt IDs defined in a LLM_Prompts.json file,
sends requests to the API, and generates a JSON file with the results.

Usage:
    python analysis_LLM.py <image_path> --prompt <prompt> --model <model_number> [--output <output_file>] [--debug]

Arguments:
    image_path: Path to the image file to be processed.
    --prompt: Comma-separated prompts or prompt IDs (e.g., 'Describe the image, P1, P2'). Use 'list' to display all prompts.
    --model: Model number for analysis (1-N) or 'list' to display all models.
    --output: Optional output file path for the JSON results.
    --debug: Enable debug logging.

Example:
    python analysis_LLM.py test.png --prompt "P1" --model 1 --output results.json

To list all available models:
    python analysis_LLM.py --model list

To list all available prompts:
    python analysis_LLM.py --prompt list
"""

import os
import logging
import requests
import argparse
import json
import base64
from typing import Optional, Dict, Any, List
from dotenv import load_dotenv

# Load environment variables from .env file
load_dotenv()

# Load status messages from .env or set default words
EMOJI_SUCCESS = os.getenv("EMOJI_SUCCESS", "SUCCESS")
EMOJI_WARNING = os.getenv("EMOJI_WARNING", "WARNING")
EMOJI_ERROR = os.getenv("EMOJI_ERROR", "ERROR")
EMOJI_INFO = os.getenv("EMOJI_INFO", "INFO")
EMOJI_PROCESSING = os.getenv("EMOJI_PROCESSING", "PROCESSING")
EMOJI_START = os.getenv("EMOJI_START", "START")
EMOJI_COMPLETE = os.getenv("EMOJI_COMPLETE", "COMPLETE")

# Load models from .env
MODELS = []
model_count = 1
while True:
    model_name = os.getenv(f'LLM_{model_count}_TITLE')
    model_api_url = os.getenv(f'LLM_{model_count}_API_URL')
    model_api_key = os.getenv(f'LLM_{model_count}_API_KEY')
    model_model = os.getenv(f'LLM_{model_count}_MODEL')
    if not model_name or not model_api_url or not model_model:
        break
    MODELS.append({
        'number': model_count,
        'name': model_name,
        'api_url': model_api_url,
        'api_key': model_api_key,
        'model': model_model
    })
    model_count += 1

# Load prompts from LLM_Prompts.json
PROMPTS_FILE = 'LLM_Prompts.json'

if not os.path.exists(PROMPTS_FILE):
    # Create a sample LLM_Prompts.json file if it doesn't exist
    sample_prompts = {
        "PROMPT1": {
            "TITLE": "Detailed Image Description",
            "PROMPT_TEXT": "You will be given an image to describe in detail. Your task is to provide a comprehensive and accurate description of the contents of the image.\n\nFollow these steps to describe the image:\n\n1. Begin by identifying the main subject or focus of the image. What immediately draws your attention?\n\n2. Describe the overall scene or setting. Is it indoors or outdoors? What time of day does it appear to be?\n\n3. Provide details about the main elements in the image, including:\n   - People: Describe their appearance, clothing, actions, and expressions\n   - Objects: Identify and describe key objects, their colors, sizes, and positions\n   - Animals: If present, describe their species, actions, and appearance\n   - Nature: Describe any natural elements like plants, trees, water bodies, or landscapes\n   - Buildings or structures: Describe their architecture, size, and condition\n\n4. Pay attention to colors, lighting, and atmosphere. How do these elements contribute to the overall mood or feel of the image?\n\n5. Describe any text or signage visible in the image.\n\n6. Note any unusual or striking features that stand out.\n\n7. If relevant, describe the composition of the image, including foreground, middle ground, and background elements.\n\n8. If the image depicts an action or event, describe what appears to be happening.\n\nProvide your description in clear, concise language. Be as objective as possible, focusing on what you can actually see rather than making assumptions or interpretations.\n\nIf any part of the image is unclear or difficult to discern, mention this in your description.\n\nIf for any reason you are unable to process or analyze the image, please state this clearly and explain why (e.g., \"I'm sorry, but I am unable to view or analyze the image provided.\").\n\nPresent your final description in detail so a blind person can see the image and an artist can paint it.",
            "TEMPERATURE": 0.7,
            "MAX_TOKENS": 1000
        },
        "PROMPT2": {
            "TITLE": "Art Critique from Multiple Perspectives",
            "PROMPT_TEXT": "You are an art critic tasked with providing a comprehensive critique of an image from multiple perspectives. Your goal is to analyze the image visually, interpret its meaning, and describe how it appears and what it inspires from different viewpoints.\n\nExamine the image carefully and provide a critique from each of the following perspectives:\n\n1. Artist\n2. Gallery owner\n3. Curator\n4. 12-year-old\n5. 19-year-old\n6. 50-year-old\n\nFor each perspective, consider the following aspects:\n- Visual elements (composition, color, style, technique)\n- Emotional impact\n- Potential meaning or symbolism\n- How it relates to current trends or historical context (if applicable)\n- Personal interpretation based on the specific perspective\n\nStructure your response as follows:\n\n<critique>\n<artist_perspective>\n[Provide the artist's critique here]\n</artist_perspective>\n\n<gallery_owner_perspective>\n[Provide the gallery owner's critique here]\n</gallery_owner_perspective>\n\n<curator_perspective>\n[Provide the curator's critique here]\n</curator_perspective>\n\n<twelve_year_old_perspective>\n[Provide the 12-year-old's critique here]\n</twelve_year_old_perspective>\n\n<nineteen_year_old_perspective>\n[Provide the 19-year-old's critique here]\n</nineteen_year_old_perspective>\n\n<fifty_year_old_perspective>\n[Provide the 50-year-old's critique here]\n</fifty_year_old_perspective>\n</critique>\n\nEnsure that each perspective's critique is distinct and reflects the unique viewpoint of that particular role or age group. Be creative and insightful in your analysis, while remaining respectful and constructive in your critiques.",
            "TEMPERATURE": 0.8,
            "MAX_TOKENS": 1500
        }
    }
    with open(PROMPTS_FILE, 'w') as f:
        json.dump(sample_prompts, f, indent=4)

with open(PROMPTS_FILE, 'r') as f:
    PROMPTS = json.load(f)

class LLMAnalyzer:
    def __init__(self, api_base_url: str, api_key: str, model: str, title: str = "", debug: bool = False):
        self.api_base_url = api_base_url
        self.api_key = api_key
        self.model = model
        self.title = title
        self.debug = debug
        if self.debug:
            logging.debug(f"{EMOJI_INFO} LLMAnalyzer initialized with model {model} and title '{title}'")

    def process_image(self, image_path: str, prompts: List[str], output_file: Optional[str] = None):
        logging.info(f"{EMOJI_PROCESSING} Processing image: {image_path}")
        with open(image_path, "rb") as image_file:
            image_data = base64.b64encode(image_file.read()).decode("utf-8")

        results = []
        for prompt in prompts:
            prompt_details = PROMPTS.get(prompt, {})
            prompt_text = prompt_details.get('PROMPT_TEXT', prompt)
            temperature = float(prompt_details.get('TEMPERATURE', 0.7))
            max_tokens = int(prompt_details.get('MAX_TOKENS', 1000))

            payload = {
                "model": self.model,
                "messages": [
                    {
                        "role": "user",
                        "content": [
                            {"type": "text", "text": prompt_text},
                            {"type": "image_url", "image_url": {"url": f"data:image/jpeg;base64,{image_data}"}}
                        ]
                    }
                ],
                "temperature": temperature,
                "max_tokens": max_tokens
            }

            headers = {
                "Authorization": f"Bearer {self.api_key}",
                "Content-Type": "application/json"
            }

            response = requests.post(self.api_base_url, headers=headers, json=payload)

            if response.status_code == 200:
                logging.info(f"{EMOJI_SUCCESS} Prompt '{prompt}' processed successfully.")
                result = response.json()
                results.append({
                    "prompt": prompt,
                    "result": result
                })
            else:
                logging.error(f"{EMOJI_ERROR} Failed to process prompt '{prompt}'. Status code: {response.status_code}")
                logging.error(f"{EMOJI_ERROR} Response: {response.text}")
                results.append({
                    "prompt": prompt,
                    "error": response.text
                })

        if output_file:
            with open(output_file, "w") as f:
                json.dump(results, f, indent=4)
            logging.info(f"{EMOJI_COMPLETE} Results saved to {output_file}")
        else:
            print(json.dumps(results, indent=4))

def list_models(models: List[Dict[str, Any]]):
    """
    Lists all available models with their information (excluding API keys).

    Args:
        models (List[Dict[str, Any]]): List of model dictionaries.
    """
    print(f"{EMOJI_INFO} Available Models:")
    for model in models:
        print(f"  {EMOJI_INFO} Model {model['number']}: {model['name']}")
        print(f"      API URL: {model['api_url']}")
        print(f"      Model: {model['model']}\n")

def list_prompts(prompts: Dict[str, Dict[str, Any]]):
    """
    Lists all available prompts defined in the LLM_Prompts.json file.

    Args:
        prompts (Dict[str, Dict[str, Any]]): Dictionary of prompt IDs and their details.
    """
    if not prompts:
        print(f"{EMOJI_WARNING} No prompts found in the LLM_Prompts.json file.")
        return

    print(f"{EMOJI_INFO} Available Prompts:")
    for prompt_id, prompt_details in sorted(prompts.items()):
        first_line = prompt_details['PROMPT_TEXT'].splitlines()[0]
        print(f"  {EMOJI_INFO} {prompt_id}: {prompt_details['TITLE']} - {first_line}... (Temperature: {prompt_details['TEMPERATURE']}, Max Tokens: {prompt_details['MAX_TOKENS']})")

def main():
    # Parse command-line arguments
    parser = argparse.ArgumentParser(
        description="Analyze an image using a selected LLM (Language Model) API and generate a JSON file with the results."
    )
    parser.add_argument(
        "image_path",
        type=str,
        nargs='?',
        help="Path to the image file to be processed."
    )
    parser.add_argument(
        "--prompt",
        type=str,
        help="Comma-separated prompts or prompt IDs (e.g., 'Describe the image, P1, P2'). Use 'list' to display all prompts."
    )
    parser.add_argument(
        "--model",
        type=str,
        help=f"Model number for analysis (1-{len(MODELS)}) or 'list' to display all models."
    )
    parser.add_argument(
        "--output",
        type=str,
        help="Optional output file path for the JSON results."
    )
    parser.add_argument(
        "--debug",
        action="store_true",
        help="Enable debug logging."
    )

    args = parser.parse_args()

    # Configure root logger
    logging.basicConfig(
        level=logging.DEBUG if args.debug else logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    )

    # Handle --model list and --prompt list
    if args.model and args.model.lower() == 'list':
        list_models(MODELS)

    if args.prompt and args.prompt.lower() == 'list':
        list_prompts(PROMPTS)

    # If either --model or --prompt is 'list', and no other arguments, exit
    if (args.model and args.model.lower() == 'list') or (args.prompt and args.prompt.lower() == 'list'):
        exit(0)

    # Ensure required arguments are provided for analysis
    if not args.image_path or not args.prompt or not args.model:
        parser.error("the following arguments are required for analysis: image_path, --prompt, --model")

    # Validate model number
    try:
        model_number = int(args.model)
        selected_model = next((model for model in MODELS if model['number'] == model_number), None)
        if not selected_model:
            logging.error(f"{EMOJI_ERROR} Invalid model number: {model_number}. Use '--model list' to see available models.")
            exit(1)
    except ValueError:
        logging.error(f"{EMOJI_ERROR} Invalid model value: {args.model}. It should be an integer between 1 and {len(MODELS)} or 'list'.")
        exit(1)

    # Initialize LLMAnalyzer
    analyzer = LLMAnalyzer(
        api_base_url=selected_model['api_url'],
        api_key=selected_model['api_key'],
        model=selected_model['model'],
        title=selected_model['name'],
        debug=args.debug
    )

    # Process the image
    prompts = [p.strip() for p in args.prompt.split(',')]
    analyzer.process_image(args.image_path, prompts, args.output)

if __name__ == "__main__":
    main()


###### FILENAME: analysis_main.py ######

import argparse
import logging
from config import Config
from clip_analyzer import CLIPAnalyzer
from llm_analyzer import LLMAnalyzer

def main():
    parser = argparse.ArgumentParser(description="Analyze images using LLM.")
    parser.add_argument("image_path", type=str, help="Path to the image file.")
    parser.add_argument("--prompt", type=str, help="The prompt to be used for analysis.")
    parser.add_argument("--model", type=int, required=True, help="Model number for analysis (1-5).")
    args = parser.parse_args()

    logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
    config = Config()

    logging.info("### Processing New Batch ###")

    if config.clip_enabled:
        logging.info("Running CLIP Analyzer...")
        clip_analyzer = CLIPAnalyzer(config)
        clip_analyzer.process_images()

    if config.enable_llm_analysis:  # Check if LLM analysis is enabled
        logging.info("Running LLM Analyzer...")
        llm_analyzer = LLMAnalyzer(config)
        llm_analyzer.process_images(args.image_path, args.prompt, args.model)  # Pass the model number

    logging.info("Finished processing")

if __name__ == "__main__":
    main()


###### FILENAME: analyzer.py ######

import os
import json
import logging
import time
from abc import ABC, abstractmethod
from typing import Dict, List, Tuple
from image_utils import generate_unique_code
import json_utils

class Analyzer(ABC):
    def __init__(self, directory: str):
        self.directory = directory
        self.image_extensions: Tuple[str, ...] = ('.jpg', '.jpeg', '.png')

    @abstractmethod
    def analyze_image(self, image_path: str) -> Dict:
        pass

    def process_directory(self) -> None:
        start_time = time.time()
        existing_files = self.get_existing_json_files()

        total_images, processed_images = self._process_images(existing_files)

        self._log_processing_summary(total_images, processed_images, start_time)

    def _process_images(self, existing_files: List[str]) -> Tuple[int, int]:
        total_images, processed_images = 0, 0

        for image_path in self.get_image_files():
            total_images += 1
            if json_utils.should_process_file(image_path, existing_files, self.__class__.__name__):
                try:
                    result = self.analyze_image(image_path)
                    self.save_result(image_path, result)
                    processed_images += 1
                    logging.info(f"Processed {processed_images}/{total_images}: {os.path.basename(image_path)}")
                except Exception as e:
                    logging.error(f"Error processing {image_path}: {e}")

        return total_images, processed_images

    def _log_processing_summary(self, total_images: int, processed_images: int, start_time: float) -> None:
        total_time = time.time() - start_time
        logging.info(f"Total processing time: {total_time:.2f} seconds")
        logging.info(f"Processed {processed_images}/{total_images} images")

    def save_result(self, image_path: str, result: Dict) -> None:
        json_path = f"{os.path.splitext(image_path)[0]}_{self.__class__.__name__}.json"
        self.ensure_output_directory(os.path.dirname(json_path))
        with open(json_path, 'w') as f:
            json.dump(result, f, indent=2)
        logging.info(f"Saved results to {json_path}")

    def ensure_output_directory(self, path: str) -> None:
        os.makedirs(path, exist_ok=True)

    def get_image_files(self) -> List[str]:
        image_files = []
        for root, _, files in os.walk(self.directory):
            for file in files:
                if file.lower().endswith(self.image_extensions):
                    image_files.append(os.path.join(root, file))
        return image_files

    def get_existing_json_files(self) -> List[str]:
        return json_utils.get_existing_json_files(self.directory)



###### FILENAME: api_utils.py ######

import logging
import time
import json
import os
from functools import wraps
from typing import Callable, Dict, Any, Optional
import requests

def log_api_conversation(logger: logging.Logger, data: Dict[str, Any], log_conversation: bool):
    if log_conversation:
        logger.debug("API Conversation:")
        logger.debug(json.dumps(data, indent=2))

def retry_with_backoff(max_retries: int = 5, initial_wait: float = 1, backoff_factor: float = 2):
    def decorator(func):
        @wraps(func)
        def wrapper(*args, **kwargs):
            retries = 0
            wait_time = initial_wait
            while retries < max_retries:
                try:
                    return func(*args, **kwargs)
                except requests.RequestException as e:
                    if isinstance(e, requests.HTTPError) and e.response.status_code == 500:
                        logging.warning(f"Server error (500). Retrying in {wait_time:.2f} seconds...")
                    else:
                        logging.warning(f"Request failed: {e}. Retrying in {wait_time:.2f} seconds...")
                    time.sleep(wait_time)
                    retries += 1
                    wait_time *= backoff_factor
            raise Exception(f"Failed after {max_retries} retries")
        return wrapper
    return decorator

@retry_with_backoff()
def send_llm_request(data: Dict[str, Any], api_key: str, api_url: str, timeout: float = 30.0) -> Dict[str, Any]:
    llm_logger = logging.getLogger('LLM_API')
    headers = {
        "Content-Type": "application/json",
        "Authorization": f"Bearer {api_key}"
    }
    try:
        response = requests.post(api_url, json=data, headers=headers, timeout=timeout)
        response.raise_for_status()
        response_data = response.json()
        log_api_conversation(llm_logger, {"request": data, "response": response_data}, True)
        return response_data
    except requests.RequestException as e:
        llm_logger.error(f"Error in LLM API request: {str(e)}")
        llm_logger.error(f"Response content: {e.response.text if e.response else 'No response content'}")
        raise

def safe_api_call(func: Callable[..., Dict[str, Any]]) -> Callable[..., Optional[Dict[str, Any]]]:
    @wraps(func)
    def wrapper(*args: Any, **kwargs: Any) -> Optional[Dict[str, Any]]:
        try:
            return func(*args, **kwargs)
        except requests.RequestException as e:
            logging.error(f"API call failed: {str(e)}")
            return None
    return wrapper


###### FILENAME: clip_analyzer.py ######

import os
import logging
import time
import requests
import json
from typing import Dict, Any, Optional
from analyzer import Analyzer
from api_utils import retry_with_backoff, log_api_conversation
from image_utils import encode_image_to_base64, generate_unique_code
import json_utils

class CLIPAnalyzer(Analyzer):
    def __init__(self, config):
        super().__init__(config.image_directory)
        self.config = config
        self.logger = logging.getLogger('CLIP_API')

    def save_json(self, data: Any, filename: str):
        try:
            with open(filename, 'w') as f:
                json.dump(data, f, indent=4)
            self.logger.info(f"Saved output to {filename}")
        except Exception as e:
            self.logger.error(f"Error saving JSON to {filename}: {str(e)}")

    @retry_with_backoff(max_retries=5, initial_wait=1, backoff_factor=2)
    def send_clip_request(self, image_base64: str, request_type: str, mode: Optional[str] = None) -> Optional[Dict[str, Any]]:
        headers = {"Content-Type": "application/json"}
        payload = {
            "image": image_base64,
            "model": self.config.clip_model_name,
        }
        
        if request_type == "prompt" and mode:
            payload["mode"] = mode

        try:
            # Mask the image content in logs
            log_payload = {**payload, "image": "[BASE64_IMAGE_CONTENT]"}
            self.logger.debug(f"Sending {request_type} request to {self.config.api_base_url}/interrogator/{request_type} with payload: {log_payload}")
            
            response = requests.post(
                f"{self.config.api_base_url}/interrogator/{request_type}",
                json=payload,
                headers=headers,
                timeout=self.config.timeout
            )
            response.raise_for_status()
            response_data = response.json()
            
            # Log the API conversation if enabled
            if self.config.log_api_conversation:
                log_api_conversation(self.logger, {"request": log_payload, "response": response_data}, self.config.log_api_conversation)
            
            return response_data
        except requests.HTTPError as e:
            self.logger.error(f"HTTP error occurred during {request_type}: {e}")
            self.logger.error(f"Response content: {e.response.text if e.response else 'No response content'}")
            return None
        except requests.RequestException as e:
            self.logger.error(f"Error in CLIP API request during {request_type}: {str(e)}")
            return None

    def analyze_image(self, image_path: str) -> Optional[Dict[str, Any]]:
        try:
            image_base64 = encode_image_to_base64(image_path)
            if image_base64 is None:
                self.logger.error(f"Failed to encode image: {image_path}")
                return None
            
            results = self.send_clip_request(image_base64, "analyze")
            if results is None:
                self.logger.error(f"Analysis failed for image: {image_path}")
                return None
            
            return {
                'file_info': self._get_file_info(image_path),
                'analysis': results
            }
        except Exception as e:
            self.logger.error(f"Error analyzing image {image_path}: {str(e)}")
            return None

    def prompt_image(self, image_path: str, mode: str) -> Optional[Dict[str, Any]]:
        try:
            image_base64 = encode_image_to_base64(image_path)
            if image_base64 is None:
                self.logger.error(f"Failed to encode image: {image_path}")
                return None

            results = self.send_clip_request(image_base64, "prompt", mode)
            if results is None:
                self.logger.error(f"Prompt generation failed for image: {image_path} with mode: {mode}")
                return None
            
            return {
                'file_info': self._get_file_info(image_path),
                'prompts': results
            }
        except Exception as e:
            self.logger.error(f"Error prompting image {image_path}: {str(e)}")
            return None

    def _get_file_info(self, image_path: str) -> Dict[str, Any]:
        return {
            'filename': os.path.basename(image_path),
            'unique_hash': generate_unique_code(image_path),
            'date_created': os.path.getctime(image_path),
            'date_processed': time.time(),
            'file_size': os.path.getsize(image_path)
        }

    def process_images(self):
        existing_files = json_utils.get_existing_json_files(self.config.output_directory)
        for image_path in self.get_image_files():
            if json_utils.should_process_file(image_path, existing_files, self.__class__.__name__, self.config):
                if self.config.clip_enabled:
                    result = self.analyze_image(image_path)
                    if result is not None:
                        self.save_result(image_path, result)
                    else:
                        self.logger.warning(f"Skipping JSON creation for {image_path} due to analysis error")
                if self.config.llm_enabled:
                    modes = self.config.selected_prompts
                    for mode in modes:
                        result = self.prompt_image(image_path, mode)
                        if result is not None:
                            self.save_result(image_path, result)
                        else:
                            self.logger.warning(f"Skipping JSON creation for {image_path} due to prompt error")

    def get_image_files(self):
        for root, _, files in os.walk(self.config.image_directory):
            for file in files:
                if file.lower().endswith(tuple(self.config.image_file_extensions)):
                    yield os.path.join(root, file)

    def save_result(self, image_path: str, result: Dict[str, Any]):
        try:
            json_path = f"{os.path.splitext(image_path)[0]}_{self.__class__.__name__}.json"
            with open(json_path, 'w') as f:
                json.dump(result, f, indent=4)
            self.logger.info(f"Saved analysis result to {json_path}")
            
            # Process JSON to TXT if enabled
            json_utils.process_json_to_txt(self.config, result, os.path.dirname(json_path))
        except Exception as e:
            self.logger.error(f"Error saving result to {json_path}: {str(e)}")




###### FILENAME: config.py ######

import os
from dotenv import load_dotenv
from typing import Dict, Any

load_dotenv()

class Config:
    """
    Configuration class for managing application settings.
    Loads settings from environment variables with default values.
    """

    def __init__(self):
        # API Keys
        self.serper_api_key = os.getenv('SERPER_API_KEY')
        self.openai_api_key = os.getenv('OPENAI_API_KEY')
        self.google_api_key = os.getenv('GOOGLE_API_KEY')
        self.google_cse_id = os.getenv('GOOGLE_CSE_ID')
        self.agentops_api_key = os.getenv('AGENTOPS_API_KEY')

        # API Configuration
        self.api_base_url = os.getenv('API_BASE_URL', 'http://localhost:7860')
        self.timeout = int(os.getenv('TIMEOUT', '60'))  # Increase to 60 seconds or more

        # Directory Settings
        self.image_directory = os.getenv('IMAGE_DIRECTORY', 'path_to_images')
        self.output_directory = os.getenv('OUTPUT_DIRECTORY', 'path_to_output')

        # Logging Configuration
        self.logging_level = os.getenv('LOGGING_LEVEL', 'INFO')
        self.logging_format = os.getenv('LOGGING_FORMAT', '%(asctime)s - %(name)s - %(levelname)s - %(message)s')
        self.log_to_console = os.getenv('LOG_TO_CONSOLE', 'True').lower() == 'true'
        self.log_to_file = os.getenv('LOG_TO_FILE', 'True').lower() == 'true'
        self.log_file = os.getenv('LOG_FILE', 'Log.log')
        self.log_mode = 'w'  # Always overwrite log file
        self.log_api_conversation = os.getenv('LOG_API_CONVERSATION', 'False').lower() == 'true'

        # Model Settings
        self.clip_model_name = os.getenv('CLIP_MODEL_NAME', 'ViT-L-14')
        self.caption_types = os.getenv('CAPTION_TYPES', 'caption,best,fast,classic,negative').split(',')

        # LLM Settings
        self.llm_api_base_url = os.getenv('LLM_API_BASE_URL', 'https://api.openai.com/v1/chat/completions')
        self.llm_model = os.getenv('LLM_MODEL', 'gpt-4')
        self.llm_system_content = os.getenv('LLM_SYSTEM_CONTENT', 'Your default system content here')

        # File Handling Settings
        self.create_individual_files = os.getenv('CREATE_INDIVIDUAL_FILES', 'True').lower() == 'true'
        self.create_prompt_list = os.getenv('CREATE_PROMPT_LIST', 'True').lower() == 'true'
        self.create_master_files = os.getenv('CREATE_MASTER_FILES', 'True').lower() == 'true'
        self.list_file_mode = os.getenv('LIST_FILE_MODE', 'w')
        self.master_analysis_filename = os.getenv('MASTER_ANALYSIS_FILENAME', 'master_analysis.json')
        self.process_json_without_images = os.getenv('PROCESS_JSON_WITHOUT_IMAGES', 'False').lower() == 'true'

        # Image File Extensions
        self.image_file_extensions = os.getenv('IMAGE_FILE_EXTENSIONS', '.png,.jpg,.jpeg').split(',')

        # LLM Configurations
        self.llms = self._load_llm_configs()

        # Analysis Control
        self.clip_enabled = os.getenv('ENABLE_CLIP_ANALYSIS', 'True').lower() == 'true'
        self.llm_enabled = os.getenv('ENABLE_LLM_ANALYSIS', 'False').lower() == 'true'
        self.enable_json_processing = os.getenv('ENABLE_JSON_PROCESSING', 'True').lower() == 'true'

        # Retry Configuration
        self.retry_limit = int(os.getenv('RETRY_LIMIT', '5'))
        self.sleep_interval = int(os.getenv('SLEEP_INTERVAL', '5'))

        # LLM Parameters
        self.temperature = float(os.getenv('TEMPERATURE', '0.7'))
        self.max_tokens = int(os.getenv('MAX_TOKENS', '300'))
        self.top_p = float(os.getenv('TOP_P', '1.0'))
        self.frequency_penalty = float(os.getenv('FREQUENCY_PENALTY', '0.0'))
        self.presence_penalty = float(os.getenv('PRESENCE_PENALTY', '0.0'))

        # Selected Prompts
        self.selected_prompts = [p for p in os.getenv('SELECTED_PROMPTS', '').split(',') if p]

        # New Analysis Modes
        self.enable_caption = os.getenv('ENABLE_CAPTION', 'True').lower() == 'true'
        self.enable_best = os.getenv('ENABLE_BEST', 'True').lower() == 'true'
        self.enable_fast = os.getenv('ENABLE_FAST', 'True').lower() == 'true'
        self.enable_classic = os.getenv('ENABLE_CLASSIC', 'True').lower() == 'true'
        self.enable_negative = os.getenv('ENABLE_NEGATIVE', 'True').lower() == 'true'

        # Additional settings
        self.process_json_to_txt = os.getenv('PROCESS_JSON_TO_TXT', 'True').lower() == 'true'

    def _load_llm_configs(self) -> Dict[str, Dict[str, Any]]:
        llms = {}
        for i in range(1, 5):  # Assuming a maximum of 4 LLM configurations
            enabled = os.getenv(f'LLM_{i}_ENABLED', 'False').lower() == 'true'
            if enabled:
                llms[f'LLM_{i}'] = {
                    'api_url': os.getenv(f'LLM_{i}_API_URL'),
                    'api_key': os.getenv(f'LLM_{i}_API_KEY'),
                }
        return llms

    def get_openai_api_key(self) -> str:
        return self.openai_api_key  # Make sure this attribute is set in the __init__ method

    def get_prompt_options(self, prompt_id: str) -> Dict[str, Any]:
        return {
            'PROMPT_TEXT': os.getenv(f'{prompt_id.upper()}_PROMPT_TEXT', 'Default prompt text'),
            'TEMPERATURE': float(os.getenv(f'{prompt_id.upper()}_TEMPERATURE', str(self.temperature))),
            'MAX_TOKENS': int(os.getenv(f'{prompt_id.upper()}_MAX_TOKENS', str(self.max_tokens)))
        }

    def _get_enabled_modes(self):
        return [mode for mode, enabled in {
            'caption': self.enable_caption,
            'best': self.enable_best,
            'fast': self.enable_fast,
            'classic': self.enable_classic,
            'negative': self.enable_negative
        }.items() if enabled]

    def __str__(self) -> str:
        return f"""
        Configuration:
        - API Base URL: {self.api_base_url}
        - Timeout: {self.timeout}
        - Image Directory: {self.image_directory}
        - Output Directory: {self.output_directory}
        - CLIP Model: {self.clip_model_name}
        - CLIP Mode: {self.clip_mode}
        - LLM Model: {self.llm_model}
        - Enable CLIP Analysis: {self.enable_clip_analysis}
        - Enable LLM Analysis: {self.enable_llm_analysis}
        - Enable JSON Processing: {self.enable_json_processing}
        """


###### FILENAME: image_utils.py ######

import base64
import hashlib
import logging
from typing import Optional
import os

def generate_unique_code(image_path: str) -> str:
    """
    Generates a unique MD5 hash for the given image file.

    Args:
        image_path (str): The path to the image file.

    Returns:
        str: The MD5 hash of the image file.
    """
    try:
        with open(image_path, "rb") as f:
            file_hash = hashlib.md5()
            chunk = f.read(8192)
            while chunk:
                file_hash.update(chunk)
                chunk = f.read(8192)
        return file_hash.hexdigest()
    except Exception as e:
        logging.error(f"Error generating unique code for {image_path}: {str(e)}")
        return f"error_{os.path.basename(image_path)}"

def encode_image_to_base64(image_path: str) -> Optional[str]:
    """
    Encodes an image file to a base64 string.

    Args:
        image_path (str): The path to the image file.

    Returns:
        Optional[str]: The base64 encoded string of the image content, or None if an error occurs.
    """
    try:
        # Check if the file exists and is accessible
        if not os.path.exists(image_path):
            logging.error(f"Image file does not exist: {image_path}")
            return None

        # Read the image file in binary mode
        with open(image_path, 'rb') as image_file:
            encoded_string = base64.b64encode(image_file.read()).decode('utf-8')
            logging.info(f"Successfully encoded image to base64: {image_path}")
            return encoded_string
    except Exception as e:
        logging.error(f"Error encoding image to base64 {image_path}: {str(e)}")
        return None

# Keep other existing functions like resize_image


###### FILENAME: json_utils.py ######

import os
import json
import logging
from typing import Dict, Any, List, Set

def save_json(file_path: str, data: Dict[str, Any]) -> None:
    try:
        if os.path.exists(file_path):
            with open(file_path, 'r', encoding='utf-8') as json_file:
                existing_data = json.load(json_file)
            
            # Update file_info
            existing_data['file_info'] = data['file_info']
            
            # Merge analysis results
            for mode, result in data['analysis'].items():
                if mode not in existing_data['analysis']:
                    existing_data['analysis'][mode] = result
                elif result['model'] != existing_data['analysis'][mode]['model']:
                    existing_data['analysis'][f"{mode}_{result['model']}"] = result
            
            data = existing_data
        
        with open(file_path, 'w', encoding='utf-8') as json_file:
            json.dump(data, json_file, ensure_ascii=False, indent=4)
        logging.info(f"JSON output created/updated: {file_path}")
    except IOError as e:
        logging.error(f"Failed to create or write to file: {e}, Path attempted: {file_path}")

def get_existing_json_files(directory: str) -> Set[str]:
    json_files = set()
    for root, _, files in os.walk(directory):
        for file in files:
            if file.lower().endswith('.json'):
                json_files.add(file)
    return json_files

def should_process_file(file_path: str, existing_files: List[str], analyzer_name: str, config) -> bool:
    json_filename = f"{os.path.splitext(os.path.basename(file_path))[0]}_{analyzer_name}.json"
    if json_filename in existing_files:
        json_path = os.path.join(config.output_directory, json_filename)
        if os.path.exists(json_path):
            with open(json_path, 'r') as f:
                existing_data = json.load(f)
            
            # Check if all caption types are already processed
            existing_types = set(existing_data.get('analysis', {}).keys())
            required_types = set(config.caption_types)
            
            if required_types.issubset(existing_types):
                logging.info(f"Skipping {file_path}, all required caption types already processed.")
                return False
    return True

def process_json_to_txt(config, json_data: Dict[str, Any], output_dir: str):
    if not config.process_json_to_txt:
        return
        return
    filename = json_data['file_info']['filename']
    base_name = os.path.splitext(filename)[0]
    base_name = os.path.splitext(filename)[0]
    for mode, content in json_data['analysis'].items():
        if isinstance(content, str):
            txt_filename = f"{base_name}_{mode}.txt"
            txt_path = os.path.join(output_dir, txt_filename)
            with open(txt_path, 'w', encoding='utf-8') as f:
                f.write(content)
            logging.info(f"Created txt file: {txt_path}")

def process_existing_json_files(config):
    logging.info("Processing existing JSON files...")
    json_files = get_existing_json_files(config.image_directory)
    
    for json_file in json_files:
        file_path = os.path.join(config.image_directory, json_file)
        try:
            with open(file_path, 'r') as f:
                data = json.load(f)
            
            # Process the JSON data as needed, but don't create any txt files
            # You can add any necessary processing logic here
            
            logging.info(f"Processed {json_file}")
        except Exception as e:
            logging.error(f"Error processing {json_file}: {str(e)}")
    
    logging.info("Finished processing existing JSON files")


###### FILENAME: logging_setup.py ######

import os
import logging

def setup_logging(config):
    """
    Setup logging configurations based on the configuration values.

    Args:
        config: Configuration object containing logging settings.

    This function sets up file and console logging, as well as specific loggers for API communication, CLIP API, and LLM API.
    """
    log_file = os.path.join(config.output_directory, 'analysis.log')
    os.makedirs(config.output_directory, exist_ok=True)

    # Create a custom logger
    logger = logging.getLogger()
    logger.setLevel(config.logging_level.upper())  # Use the logging level from config

    # Create handlers
    handlers = []
    if config.log_to_file:
        f_handler = logging.FileHandler(log_file, mode='a', encoding='utf-8')
        f_handler.setLevel(logging.DEBUG)
        handlers.append(f_handler)
    if config.log_to_console:
        c_handler = logging.StreamHandler()
        c_handler.setLevel(logging.INFO)
        handlers.append(c_handler)

    # Create a consistent formatter with short time format
    formatter = logging.Formatter(
        fmt='%(asctime)s - %(levelname)s - %(name)s - %(message)s',
        datefmt='%H:%M:%S'
    )

    for handler in handlers:
        handler.setFormatter(formatter)
        logger.addHandler(handler)

    # Suppress logging from PIL and other noisy libraries
    logging.getLogger('PIL').setLevel(logging.WARNING)
    logging.getLogger('urllib3').setLevel(logging.WARNING)  # Suppress urllib3 DEBUG logs

    # Setup API logging
    if config.log_to_file:
        api_logger = logging.getLogger('API')
        api_logger.setLevel(logging.DEBUG)
        api_handler = logging.FileHandler(os.path.join(config.output_directory, 'api_communication.log'), mode='w', encoding='utf-8')
        api_handler.setFormatter(formatter)
        api_logger.addHandler(api_handler)

        # Setup CLIP API logger
        clip_logger = logging.getLogger('CLIP_API')
        clip_logger.setLevel(logging.DEBUG)
        clip_handler = logging.FileHandler(os.path.join(config.output_directory, 'api_clip.log'), mode='w', encoding='utf-8')
        clip_handler.setFormatter(formatter)
        clip_logger.addHandler(clip_handler)

        # Setup LLM API logger if needed
        llm_logger = logging.getLogger('LLM_API')
        llm_logger.setLevel(logging.DEBUG)
        llm_handler = logging.FileHandler(os.path.join(config.output_directory, 'api_llm.log'), mode='w', encoding='utf-8')
        llm_handler.setFormatter(formatter)
        llm_logger.addHandler(llm_handler)



###### FILENAME: TEST.py ######

import requests  # Import the requests library to make HTTP requests
import base64  # Import base64 for encoding images
import os  # Import os for file path operations
import json  # Import json for handling JSON data

def encode_image_to_base64(image_path: str) -> str:
    """
    Encodes an image file to a base64 string.

    Args:
        image_path (str): The path to the image file.

    Returns:
        str: The base64 encoded string of the image.
    """
    with open(image_path, "rb") as image_file:  # Open the image file in binary read mode
        return base64.b64encode(image_file.read()).decode('utf-8')  # Encode the image and decode to UTF-8 string

def save_json(data, filename: str):
    """
    Saves data to a JSON file.

    Args:
        data: The data to save (usually a dictionary).
        filename (str): The name of the file to save the data to.
    """
    with open(filename, 'w') as f:  # Open the file in write mode
        json.dump(data, f, indent=4)  # Write the data to the file in JSON format with indentation
    print(f"Saved output to {filename}")  # Print confirmation of save

def analyze_image(image_path: str, api_base_url: str, model: str, timeout: int = 60):
    """
    Sends an image to the CLIP API for analysis.

    Args:
        image_path (str): The path to the image file.
        api_base_url (str): The base URL of the API.
        model (str): The model name to use for analysis.
        timeout (int): The timeout for the API request (default is 60 seconds).

    Returns:
        dict: The JSON response from the API containing analysis results.
    """
    image_base64 = encode_image_to_base64(image_path)  # Encode the image to base64
    
    # Prepare the payload for the API request
    payload = {
        "image": image_base64,  # The base64 encoded image
        "model": model  # The model to use for analysis
    }
    
    headers = {"Content-Type": "application/json"}  # Set the content type to JSON
    
    # Make a POST request to the API for analysis
    response = requests.post(
        f"{api_base_url}/interrogator/analyze",  # API endpoint for analysis
        json=payload,  # The payload containing the image and model
        headers=headers,  # The headers for the request
        timeout=timeout  # Timeout for the request
    )
    response.raise_for_status()  # Raise an error for bad responses (4xx or 5xx)
    return response.json()  # Return the JSON response from the API

def prompt_image(image_path: str, api_base_url: str, model: str, mode: str, timeout: int = 60):
    """
    Sends an image to the CLIP API to generate a prompt.

    Args:
        image_path (str): The path to the image file.
        api_base_url (str): The base URL of the API.
        model (str): The model name to use for generating prompts.
        mode (str): The mode for prompt generation (e.g., 'fast', 'best').
        timeout (int): The timeout for the API request (default is 60 seconds).

    Returns:
        dict: The JSON response from the API containing prompt results.
    """
    image_base64 = encode_image_to_base64(image_path)  # Encode the image to base64
    
    # Prepare the payload for the API request
    payload = {
        "image": image_base64,  # The base64 encoded image
        "model": model,  # The model to use for generating prompts
        "mode": mode  # The mode for prompt generation
    }
    
    headers = {"Content-Type": "application/json"}  # Set the content type to JSON
    
    # Make a POST request to the API for prompt generation
    response = requests.post(
        f"{api_base_url}/interrogator/prompt",  # API endpoint for prompt generation
        json=payload,  # The payload containing the image, model, and mode
        headers=headers,  # The headers for the request
        timeout=timeout  # Timeout for the request
    )
    response.raise_for_status()  # Raise an error for bad responses (4xx or 5xx)
    return response.json()  # Return the JSON response from the API

def main():
    # Configuration
    api_base_url = "http://localhost:7860"  # Change if your API runs on a different URL
    model = "ViT-L-14"  # Change to the model you are using
    mode = "fast"  # Mode for prompt; can be 'fast', 'best', 'classic', 'negative', 'caption'
    image_filename = "test.png"  # Replace with your image filename
    
    # Check if image exists
    if not os.path.isfile(image_filename):  # Verify that the image file exists
        print(f"Image file {image_filename} not found in the current directory.")
        return
    
    # Analyze the image
    try:
        analysis_result = analyze_image(image_filename, api_base_url, model)
        save_json(analysis_result, "analyze_output.json")
    except Exception as e:
        print(f"An error occurred during analysis: {e}")
    
    # Generate prompt from the image
    try:
        prompt_result = prompt_image(image_filename, api_base_url, model, mode)
        save_json(prompt_result, "prompt_output.json")
    except Exception as e:
        print(f"An error occurred during prompt generation: {e}")

if __name__ == "__main__":
    main()


###### FILENAME: utils.py ######

import base64
import hashlib
import requests
import os
import logging
from functools import wraps
from PIL import Image
import io
from typing import Optional, Dict, Any, Callable

def safe_api_call(func: Callable) -> Callable:
    """
    Decorator to safely handle API calls and log errors.
    """
    @wraps(func)
    def wrapper(*args, **kwargs):
        try:
            return func(*args, **kwargs)
        except requests.RequestException as e:
            logging.error(f"API request failed: {str(e)}")
            return {"error": str(e)}
        except Exception as e:
            logging.exception(f"Unexpected error in API call: {str(e)}")
            return {"error": "An unexpected error occurred"}
    return wrapper

def validate_directory(directory: str) -> None:
    """
    Validate if a directory exists and is accessible.
    """
    if not os.path.isdir(directory):
        raise ValueError(f"The specified path is not a valid directory: {directory}")
    if not os.access(directory, os.R_OK):
        raise PermissionError(f"You don't have read permissions for the directory: {directory}")


